spray.can.server {
	# uncomment the next line for making this an HTTPS example
	# ssl-encryption = on
	idle-timeout = 301 s
	request-timeout = 300 s
}

remote{
	akka {
		//loglevel = "DEBUG"
		actor {
			provider = "akka.remote.RemoteActorRefProvider"
		}
		remote {
			enabled-transports = ["akka.remote.netty.tcp"]
			log-sent-messages = on
			log-received-messages = on
			netty.tcp {
				transport-class = "akka.remote.transport.netty.NettyTransport"
				hostname = "localhost"
				port = 4042
			}
		}
	}
}

############ spark configuration - see spark documentation ####################
sparkConfiguration {
	spark-executor-memory=2g
	spark-mesos-coarse=false
	spark-scheduler-mode=FAIR
	spark-cores-max=2
	spark-master="spark://localhost:7077"
	spark-path="/Users/cristianionitoiu/work/spark-1.6.0-bin-hadoop2.6"
	# spark-mesos-executor-home="/home/ubuntu/latest-mssh/spark-1.1.0"
	spark-default-parallelism=384
	spark-storage-memoryFraction=0.3
	spark-shuffle-memoryFraction=0.6
	spark-shuffle-compress=true
	spark-shuffle-spill-compress=true
	spark.reducer.maxSizeInFlight=48
	spark-akka-frameSize=2000
	spark-akka-threads=4
	spark-akka-timeout=100
	spark-task-maxFailures=4
	spark-shuffle-consolidateFiles=true
	spark-deploy-spreadOut=true
	spark-shuffle-spill=false
	#Serialization settings commented until more tests are performed
	#spark-serializer="org.apache.spark.serializer.KryoSerializer"
	#spark-kryoserializer-buffer-mb=10
	#spark-kryoserializer-buffer-max-mb=64
	#spark-kryo-referenceTracking=false
}

######### application configuration ###################
appConf{
	# the interface on which to start the spray server : localhost/ip/hostname
	server.interface=localhost
	# the cors filter allowed hosts
	cors-filter-allowed-hosts="*"
	# the default number of results retrieved on queries
	nr.of.results=100
	# the ip of the destination namenode - it is used when querying with unlimited number of results.
	rdd.destination.ip="localhost"
	# where to store the results in the case of an unlimited query. Possible results : hdfs/tachyon. Default hdfs
	rdd.destination.location=hdfs
	# the remote doamain actor address
	remote.domain.actor=""
	#remote.domain.actor="devbox.local:port,devbox2.local:port"
	# application name
	application.name="Jaws"
	# the port on which to deploy the apis
	web.services.port=9080
	# the port on which to deploy the web sockets api (logs)
	web.sockets.port=8182
	# the number of threads used to execute shark commands
	nr.of.threads=10
	# implicit akka timeout
	timeout=1000000
	#where to log, save results and keep parquet - dal type = cassandra/hdfs
	app.dal.type=cassandra
	# folder where to write the results schema
	schemaFolder=jawsSchemaFolder
	# the path to the xpatterns-jaws in target folder
	jar-path="/Users/cionitoiu/work/jaws/jaws-spark-sql-rest/target/jaws-spark-sql-rest.jar,/Users/cristianionitoiu/work/xpatterns-spark-parquet/spark-parquet-utility/target/spark-parquet-utility.jar"
	# the path to the hdfs namenode
	hdfs-namenode-path="hdfs://localhost:8020"
	# the path to the tachyon namenode
	tachyon-namenode-path="tachyon://localhost:19998"
}

########## hadoop configuration - used to built HIVEContext and for HDFS Dal (if selected) ########
hadoopConf {
	namenode="hdfs://localhost:8020"
	replicationFactor=1
	# set on true if you want to start fresh (all the existing folders will be recreated)
	forcedMode=false
	# folder where to write the logs
	loggingFolder=jawsLogs
	# folder where to write the jobs states
	stateFolder=jawsStates
	# folder where to write the jobs details
	detailsFolder=jawsDetails
	# folder where to write the jobs results
	resultsFolder=jawsResultsFolder
	# folder where to write the jobs meta information
	metaInfoFolder=jawsMetainfoFolder
	# folder where to write the name of query information
	queryNameFolder=jawsQueryNameFolder
	# folder where to write the published queries
	queryPublishedFolder=jawsQueryPublishedFolder
	# folder where to write the unpublished queries
	queryUnpublishedFolder=jawsQueryUnpublishedFolder
	# folder where to write the parquet tables information
	parquetTablesFolder=parquetTablesFolder
}

########## cassandra configuration - skip this if you are using hdfs DAL ##########
cassandraConf {
	cassandra.host="localhost:9160"
	cassandra.keyspace=xpatterns_jaws_xt
	cassandra.cluster.name=Jaws
}

############# Kerberos configuration ##################
kerberosConf {
	principal.id="cristiani@STAGING.XPATTERNS.COM"
	keytabfile.path = "/Users/cristianionitoiu/cristiani.keytab"
	kerberos.realm = "STAGING.XPATTERNS.COM"
	kerberos.ip = "10.0.2.181"
}